
import os
import re
import torch
import faiss
import sys
import urllib.request
import json
import folium
import numpy as np
import pandas as pd
import streamlit as st
from datetime import datetime
import google.generativeai as genai

from tqdm import tqdm
from streamlit_folium import st_folium
from folium.plugins import Search
from PIL import Image
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification


########################## ì„ë² ë”© ì½”ë“œ ###############################
# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'
index_path = os.path.join(module_path, 'faiss_index.index')
embedding_array_path = os.path.join(module_path, 'embeddings_array_file.npy')

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "intfloat/multilingual-e5-large-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

# í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()

# FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
def initialize_faiss_index(dimension=1024):
    if os.path.exists(index_path):
        return faiss.read_index(index_path)
    else:
        return faiss.IndexFlatL2(dimension)

# ì „ì²´ ë°ì´í„°ë¡œ FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì„ë² ë”© ë°°ì—´ ì €ì¥
def create_vector_db(dataframe, index, save=True):
    embeddings = np.array([embed_text(text) for text in dataframe['text']])
    print(f"ìƒì„±ëœ ì„ë² ë”© ë°°ì—´ í¬ê¸°: {embeddings.shape}")
    index.add(embeddings)
    if save:
        faiss.write_index(index, index_path)
        np.save(embedding_array_path, embeddings)  # ì„ë² ë”© ë°°ì—´ ì €ì¥
    print(f"FAISS ì¸ë±ìŠ¤ ë° ì„ë² ë”© ë°°ì—´ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ FAISS ì¸ë±ìŠ¤ì™€ ì„ë² ë”© ë°°ì—´ ì—…ë°ì´íŠ¸
def update_vector_db(new_data, index, save=True):
    # ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„±
    new_embeddings = np.array([embed_text(text) for text in new_data['text']])
    
    # ê¸°ì¡´ ì„ë² ë”© ë°°ì—´ ë¶ˆëŸ¬ì˜¤ê¸°
    if os.path.exists(embedding_array_path):
        existing_embeddings = np.load(embedding_array_path)
        updated_embeddings = np.vstack([existing_embeddings, new_embeddings])
    else:
        updated_embeddings = new_embeddings  # ìµœì´ˆ ì‹¤í–‰ ì‹œ

    # FAISS ì¸ë±ìŠ¤ì™€ ì„ë² ë”© ë°°ì—´ ê°±ì‹ 
    index.add(new_embeddings)
    if save:
        faiss.write_index(index, index_path)               # ì¸ë±ìŠ¤ íŒŒì¼ ê°±ì‹ 
        np.save(embedding_array_path, updated_embeddings)  # ì„ë² ë”© ë°°ì—´ ê°±ì‹ 
    print(f"FAISS ì¸ë±ìŠ¤ì™€ ì„ë² ë”© ë°°ì—´ì´ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.")

# CSV íŒŒì¼ ë¡œë“œ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”
csv_file_path = "JEJU_MCT_DATA_modified_v8.csv"
df = pd.read_csv(os.path.join(data_path, csv_file_path))

# # ìµœì´ˆ ì‹¤í–‰ ì‹œ ì „ì²´ ë°ì´í„°ë¡œ ì¸ë±ìŠ¤ ìƒì„±
faiss_index = initialize_faiss_index()
print(f"ì´ˆê¸°í™”ëœ FAISS ì¸ë±ìŠ¤ì˜ ë²¡í„° ê°œìˆ˜: {faiss_index.ntotal}")  # ì¶”ê°€ëœ ë¡œê·¸

# ì„ë² ë”© ë¡œë“œ ë˜ëŠ” ìƒì„±
if os.path.exists(embedding_array_path):
    embeddings = np.load(embedding_array_path)
    print(f"ì„ë² ë”© ë°°ì—´ì´ {embedding_array_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("ì„ë² ë”© ë°°ì—´ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    embeddings = np.array([embed_text(text) for text in df['text']])
    np.save(embedding_array_path, embeddings)  # ìƒì„±ëœ ì„ë² ë”© ë°°ì—´ ì €ì¥


# í˜„ì¬ ì›”ì„ "2023ë…„ MMì›”" í˜•ì‹ìœ¼ë¡œ ì„¤ì •
current_year_month = f"2023ë…„ {datetime.now().strftime('%mì›”')}"

# í•„í„°ë§ëœ ë°ì´í„° ì¸ë±ìŠ¤ ì¶”ì¶œ ë° í•„í„°ë§ëœ FAISS ì¸ë±ìŠ¤ ìƒì„±
df_filtered = df[df['ê¸°ì¤€ì—°ì›”'] == current_year_month].reset_index()
print(df_filtered)

print(f"ì‹œìŠ¤í…œ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •ëœ ê¸°ì¤€ì—°ì›”: {current_year_month}")


# í•„í„°ë§ëœ ë°ì´í„° ì¸ë±ìŠ¤ ì¶”ì¶œ ë° í•„í„°ë§ëœ FAISS ì¸ë±ìŠ¤ ìƒì„±
original_indices = df_filtered['index'].values
filtered_embeddings = embeddings[original_indices]
filtered_dimension = filtered_embeddings.shape[1]
filtered_faiss_index = faiss.IndexFlatL2(1024)
filtered_faiss_index.add(filtered_embeddings)

####################################################################


# rerankë¥¼ ìœ„í•œ ëª¨ë¸ ë¡œë“œ
# í•œêµ­ì–´ reranker ëª¨ë¸ ë¡œë“œ
rerank_model_path = "Dongjin-kr/ko-reranker"
rerank_tokenizer = AutoTokenizer.from_pretrained(rerank_model_path)
rerank_model = AutoModelForSequenceClassification.from_pretrained(rerank_model_path)


####################################################################

# ë„¤ì´ë²„ API ì„¤ì •
client_id = st.secrets["CLIENT_ID"]
client_secret = st.secrets["CLIENT_SECRET"]

# êµ¬ê¸€ API ì„¤ì •
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
genai.configure(api_key=GOOGLE_API_KEY)

# Gemini ëª¨ë¸ ì„ íƒ
model = genai.GenerativeModel("gemini-1.5-flash")



# Streamlit App UI
st.set_page_config(page_title="ğŸŠì œì£¼í•œì…ë¡œë“œğŸŠ")

# í—¤ë” í‘œì‹œ
st.markdown('<div class="header"><h1>ğŸš¨404 ë§›ì§‘ Not Found (Error: \"Hungry\")ğŸš¨</h1></div>', unsafe_allow_html=True)

# ì„ íƒëœ ì´ë¯¸ì§€ ì—¬ë¶€ í™•ì¸
if 'local_choice' not in st.session_state:
    st.session_state.messages = []  # ë©”ì‹œì§€ ì´ˆê¸°í™” ì¶”ê°€
    st.markdown("<p>ëˆ„êµ¬ì™€ í•¨ê»˜ ì œì£¼ ë§›ì§‘ ì—¬í–‰ì„ ë– ë‚ ê¹Œìš”? <strong>í•´ë…€, í•˜ë¥´ë°©, ì œì£¼ì†Œë…„, ì œì£¼ì†Œë…€</strong> ì¤‘ í•œ ëª…ì„ ì„ íƒí•´ ì£¼ì„¸ìš”!</p>", unsafe_allow_html=True)
    st.markdown("<p><strong>even</strong>í•˜ê²Œ ì¶”ì²œí•´ë“œë¦´ê²Œìš”ğŸ¤­</p>", unsafe_allow_html=True)  # ì´ˆê¸° í™”ë©´ì—ë§Œ í‘œì‹œ

    # ì´ë¯¸ì§€ URL ê²½ë¡œ
    haenyeo_img_url = "https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMjExMDNfMTIw%2FMDAxNjY3NDQxMDk1Mzc2.jJcIh2j0p29Umkbzj9cljZRkVwNpJrxGih6-WD7Eat0g.U1UQ92W_M-4-DT1b5xqu0kCT3QfkXBdBZ0zx-wVaYRYg.JPEG.aewolbakery%2F%25BE%25D6%25BF%25F9%25C0%25CC.jpg&type=sc960_832"
    harbang_img_url = "https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAxNzA0MjdfMTE5%2FMDAxNDkzMjcyMTc4NzU2.p4VcaOTPB67-86psVgjyRCP2LcoWEcqLKy5cIz4loE4g.2aKY_hkrF6TzhLg2HlIzz21HJ_yy6UBaaYnvj2bXSW4g.JPEG.jeagu74%2F%25B5%25B9%25C7%25CF%25B7%25E7%25B9%25E6.jpg&type=sc960_832"
    jeju_sonya_img_url = "https://search.pstatic.net/common/?src=http%3A%2F%2Fshop1.phinf.naver.net%2F20201218_47%2F160829080654641XyY_JPEG%2F9426590261359007_1249152632.jpg&type=sc960_832"
    jeju_sonyun_img_url = "https://search.pstatic.net/common/?src=http%3A%2F%2Fblogfiles.naver.net%2FMjAyMjExMDRfMzUg%2FMDAxNjY3NTQyMzAwNjE2.ej8C7N7WYFZzXFZZnLdWhO4e7MprfvrLCcjMQYtiPkcg.1eblu8ZhWRvwjl_E7otYou-YIV0pds-BZpyCdwjwgwsg.JPEG.aewolbakery%2F%25C7%25CF%25B7%25E7%25BB%25A7.jpg&type=sc960_832"

    # CSS ìŠ¤íƒ€ì¼ ì •ì˜
    st.markdown(
        """
        <style>
        .image-container {
            display: flex;
            justify-content: space-around;
            margin-top: 20px;
        }
        .image-container img {
            width: 150px;
            height: 150px;
            border-radius: 10px;
            transition: transform 0.2s;
        }
        .image-container img:hover {
            transform: scale(1.1);
        }
        .button {
            display: flex;
            justify-content: center;
            margin-top: 10px;
        }
        .tagline {
            text-align: center;
            margin-top: 5px;
            font-size: 14px;
            color: #666;
        }
        </style>
        """, 
        unsafe_allow_html=True
    )

    # ì´ë¯¸ì§€ì™€ ë²„íŠ¼ì„ í¬í•¨í•œ HTML êµ¬ì¡°
    st.markdown(
        f"""
        <div class="image-container">
            <div>
                <img src="{haenyeo_img_url}" alt="í•´ë…€">
                <div class="tagline">#ì¹œê·¼í•¨ #ì‹¤ìš©ì  #í’ë¶€í•œ ê²½í—˜</div>
            </div>
            <div>
                <img src="{harbang_img_url}" alt="í•˜ë¥´ë°©">
                <div class="tagline">#ëŠê¸‹í•¨ #í¸ì•ˆí•¨ #ì¹œì ˆí•¨</div>
            </div>
            <div>
                <img src="{jeju_sonya_img_url}" alt="ì œì£¼ì†Œë…€">
                <div class="tagline">#íŠ¸ë Œë””í•¨ #ê°ê°ì </div>
            </div>
            <div>
                <img src="{jeju_sonyun_img_url}" alt="ì œì£¼ì†Œë…„">
                <div class="tagline">#ì¬ì¹˜ #í™œë°œí•¨ #ë„ì „ì </div>
            </div>
        </div>
        """, 
        unsafe_allow_html=True
    )

    col1, col2, col3, col4 = st.columns(4)

    # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸
    with col1: 
        if st.button("í•´ë…€", key="haenyeo_btn"):
            st.session_state.local_choice = "í•´ë…€"
            st.session_state.messages = [{"role": "assistant", "content": "ë‚´ ê²½í—˜ìœ¼ë¡œ ì§„ì§œ ë§›ìˆëŠ” ë°ë§Œ ì†Œê°œí•´ì¤ì„œ! ì œì£¼ ë§› í•œê» ëŠë¼ë© ì˜ë„ ë“œì…”ë³´ì¿ ë‹¤!"}]
            st.rerun()  # í˜ì´ì§€ ìƒˆë¡œ ê³ ì¹¨

    with col2:
        if st.button("í•˜ë¥´ë°©", key="harbang_btn"):
            st.session_state.local_choice = "í•˜ë¥´ë°©"
            st.session_state.messages = [{"role": "assistant", "content": "ì œì£¼ ë§›ì§‘ ê¶ê¸ˆí•˜ì‹  ê±° ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´ì£¼ì‹­ì„œ. ë§›ìˆëŠ” ì§‘ë“¤ ëŠê¸‹í•˜ê²Œ ì•Œë ¤ë“œë¦´ í…Œë‹ˆ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì‹­ì„œ!"}]
            st.rerun()  # í˜ì´ì§€ ìƒˆë¡œ ê³ ì¹¨

    with col3:
        if st.button("ì œì£¼ì†Œë…€", key="jeju_sonya_btn"):
            st.session_state.local_choice = "ì œì£¼ì†Œë…€"
            st.session_state.messages = [{"role": "assistant", "content": "ì œì£¼ë„ ë§›ì§‘ ì–´ë””ì„œ ë¨¹ì–´ì•¼ í• ê¹Œìš”? ì œê°€ ëŒ€ë°• ë§›ì§‘ ëª‡ êµ°ë° ì•Œë ¤ì¤„ê²Œìš”! ì¸ìŠ¤íƒ€ì— ìë‘í•˜ëŸ¬ ê°€ìš”!â€"}]
            st.rerun()  # í˜ì´ì§€ ìƒˆë¡œ ê³ ì¹¨

    with col4:
        if st.button("ì œì£¼ì†Œë…„", key="jeju_sonyun_btn"):
            st.session_state.local_choice = "ì œì£¼ì†Œë…„"
            st.session_state.messages = [{"role": "assistant", "content": "ì œì£¼ë„ì—ì„œ ë§›ìˆëŠ” ê±° ë­ ë¨¹ì„ë˜? ë‚´ê°€ ìˆ¨ê²¨ë‘” ë§›ì§‘ë“¤ ì•Œë ¤ì¤„ê²Œ, ìš°ì„  ë„ì „í•´ë³¼ë˜?ğŸ˜œ"}]
            st.rerun()  # í˜ì´ì§€ ìƒˆë¡œ ê³ ì¹¨
else:
    # ì‚¬ì´ë“œë°” ë° ë©”ì¸ í™”ë©´ì„ ë³´ì—¬ì£¼ëŠ” ë¶€ë¶„
    with st.sidebar:
        st.title("ì œì£¼ ë§›ì§‘, ê°€ë³´ìê³ !ğŸŠ")
        st.write("")
        franchise = st.selectbox("í”„ëœì°¨ì´ì¦ˆ í¬í•¨ ì—¬ë¶€", ["í”„ëœì°¨ì´ì¦ˆ í¬í•¨", "í”„ëœì°¨ì´ì¦ˆ ë¯¸í¬í•¨"], key="franchise")

    # ì„ íƒí•œ ìŠ¤íƒ€ì¼ì— ë§ê²Œ ë©”ì¸ í™”ë©´ ì—…ë°ì´íŠ¸
    if st.session_state.local_choice in ["í•˜ë¥´ë°©"]:
        st.subheader(f"ğŸ—¿{st.session_state.local_choice}ì˜ ì¶”ì²œğŸ—¿")
    elif st.session_state.local_choice in ["ì œì£¼ì†Œë…„"]:
        st.subheader(f"ğŸ‘¦ğŸ»{st.session_state.local_choice}ì˜ ì¶”ì²œğŸ‘¦ğŸ»")
    elif st.session_state.local_choice in ["ì œì£¼ì†Œë…€"]:
        st.subheader(f"ğŸ‘§ğŸ»{st.session_state.local_choice}ì˜ ì¶”ì²œğŸ‘§ğŸ»")
    elif st.session_state.local_choice in ["í•´ë…€"]:
        st.subheader(f"ğŸ¦­{st.session_state.local_choice}ì˜ ì¶”ì²œğŸ¦­")

    if st.session_state.local_choice in ["í•˜ë¥´ë°©"]:
        st.write(f"í—ˆí—ˆ, ì œì£¼ë„ ì™€ì‹  ê±° í™˜ì˜í•©ë‹ˆë”!ğŸ‘´ğŸ» ë‚œ ì œì£¼ {st.session_state.local_choice}ì´ìš°ë‹¤!")
    elif st.session_state.local_choice in ["ì œì£¼ì†Œë…„"]:
        st.write(f"ì•ˆë…•! {st.session_state.local_choice}ì´ì•¼!")
    elif st.session_state.local_choice in ["ì œì£¼ì†Œë…€"]:
        st.write(f"ì•¼! {st.session_state.local_choice}ì•¼ğŸ‘§ğŸ»")
    elif st.session_state.local_choice in ["í•´ë…€"]:
        st.write(f"ì•ˆë…•í•˜ìš°ê½ˆ!ğŸ‘‹ğŸ» {st.session_state.local_choice}ì´ìš°ë‹¤!")
    
    # ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥
    if "messages" in st.session_state and st.session_state.messages:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])
    else:
        st.write("ì±„íŒ… ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒ ì‹œì‘í•´ë³´ì„¸ìš”!")

    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™” í•¨ìˆ˜
    def clear_chat_history():
        if st.session_state.local_choice == "í•´ë…€":
            st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”~ ìš”ì¦˜ ë­ê°€ ë§›ë‚˜ì§€? ì¶”ì²œí•´ë“œë¦´ê²Œìš”!"}]
        elif st.session_state.local_choice == "í•˜ë¥´ë°©":
            st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì œì£¼ì—ì„  ë§›ìˆëŠ” ì§‘ì´ ë§ì§€ìš”. ì–´ë–¤ ê±¸ ì°¾ê³  ê³„ì‹ ê°€ìš”?"}]
        elif st.session_state.local_choice == "ì œì£¼ì†Œë…€":
            st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•~! ë§›ìˆëŠ” ì œì£¼ ë°¥ì§‘ì„ ì†Œê°œí• ê²Œ!"}]
        elif st.session_state.local_choice == "ì œì£¼ì†Œë…„":
            st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì œì£¼ ë§›ì§‘ ì°¾ëŠ” ë° ë„ì™€ë“œë¦´ê²Œìš”!"}]
    
    st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
    def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
        if os.path.exists(index_path):
            index = faiss.read_index(index_path)
            print(f"FAISS ì¸ë±ìŠ¤ê°€ {index_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"FAISS ì¸ë±ìŠ¤ì— ì €ì¥ëœ ë²¡í„° ê°œìˆ˜: {index.ntotal}")
            return index
        else:
            raise FileNotFoundError(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
    faiss_index = load_faiss_index()

    # í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
    def embed_text(text):
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
        with torch.no_grad():
            embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
        # print(f"ê²€ìƒ‰ ì¿¼ë¦¬ì˜ ì„ë² ë”© ë²¡í„°: {embeddings}, ë²¡í„° í¬ê¸°: {embeddings.shape}")  # ì„ë² ë”© í™•ì¸
        return embeddings.squeeze().cpu().numpy()
    
    
    # ''' ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰'''
    # ë„¤ì´ë²„ ê²°ê³¼ì™€ ë¡œì»¬ ë°ì´í„° ë¹„êµ
    def filter_naver_results_with_local_data(naver_results, local_data=df_filtered):
        if naver_results is None:
            return []
    
        # ë„¤ì´ë²„ API ê²°ê³¼ì—ì„œ HTML íƒœê·¸ ì œê±°, ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì œê±°, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
        naver_names = [re.sub('<[^<]+?>', '', result['title']).strip().lower() for result in naver_results]
        naver_names = [re.sub(r'[^\w\s]', '', name) for name in naver_names]  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        
        # ë¡œì»¬ ë°ì´í„°ì˜ ê°€ë§¹ì ëª…ë„ ë™ì¼í•˜ê²Œ ì „ì²˜ë¦¬
        local_data['ê°€ë§¹ì ëª…_ì „ì²˜ë¦¬'] = local_data['ê°€ë§¹ì ëª…'].apply(lambda x: re.sub(r'[^\w\s]', '', x).strip().lower())  # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì œê±°, ì†Œë¬¸ìë¡œ ë³€í™˜
    
        print(f"ë„¤ì´ë²„ ì „ì²˜ë¦¬ëœ ìƒí˜¸ëª…: {naver_names}")
    
        # 1. ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ë¨¼ì € í•„í„°ë§
        exact_match_df = local_data[local_data['ê°€ë§¹ì ëª…_ì „ì²˜ë¦¬'].isin(naver_names)]
    
        # 2. ì •í™•í•œ ì¼ì¹˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´, í¬í•¨ ì—¬ë¶€ë¡œ í•„í„°ë§
        if exact_match_df.empty:
            filtered_df = local_data[local_data['ê°€ë§¹ì ëª…_ì „ì²˜ë¦¬'].apply(
                lambda local_name: any(local_name in naver_name or naver_name in local_name for naver_name in naver_names)
            )].reset_index(drop=True)
        else:
            filtered_df = exact_match_df.reset_index(drop=True)
    
        print("ë„¤ì´ë²„ APIì™€ ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ëŠ” ë¡œì»¬ ë°ì´í„°: ", filtered_df)
    
        # í•„í„°ëœ ë¡œì»¬ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (API ê²°ê³¼ í˜•ì‹ê³¼ ë™ì¼í•˜ê²Œ)
        filtered_data = [
            {
                'title': row['ê°€ë§¹ì ëª…'], 
                'category': row['ì—…ì¢…'], 
                'address': row['ì£¼ì†Œ'], 
                'description': row['ì„¤ëª…']
            }
            for idx, row in filtered_df.iterrows()
        ]
        
        return filtered_data  # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    
    # ìƒˆë¡œìš´ API ê²°ê³¼ ì¤‘ ë¡œì»¬ ë°ì´í„°ì— ì—†ëŠ” í•­ëª©ë§Œ FAISSì— ì¶”ê°€
    def add_new_api_results_to_faiss(api_results, filtered_faiss_index, local_data=df_filtered):
        # ì¤‘ë³µ ì—¬ë¶€ë¥¼ ì²´í¬í•˜ê¸° ìœ„í•´ ë¡œì»¬ ë°ì´í„°ì™€ ë¹„êµ
        filtered_api_data = filter_naver_results_with_local_data(api_results, local_data)
    
        # ì¤‘ë³µë˜ì§€ ì•Šì€ ë°ì´í„°ë§Œ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
        new_data = []
        for result in filtered_api_data:
            if isinstance(result, dict):
                text = f"{result['title']} {result['category']} {result['address']} {result.get('description', '')}"
                embedding = embed_text(text)
                filtered_faiss_index.add(np.array([embedding]))
                new_data.append(result)  # ìƒˆë¡œìš´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            else:
                # resultê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê³  ê±´ë„ˆëœ€
                print(f"Warning: Unexpected data type {type(result)} for result: {result}")
        
        print(f"ìƒˆë¡œìš´ API ë°ì´í„° {len(new_data)}ê±´ì´ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        if new_data:
            # ì¶”ê°€ëœ ë°ì´í„°ë¥¼ ì¶œë ¥í•´ í™•ì¸
            print(f"ì¶”ê°€ëœ ë°ì´í„°: {new_data}")
            return pd.DataFrame(new_data)
        else:
            return pd.DataFrame()  # ë¹ˆ DataFrame ë°˜í™˜
    
    # ì„ë² ë”© ë¡œë“œ
    embeddings = np.load(os.path.join(module_path, 'embeddings_array_file.npy'))
    
    # ì§ˆë¬¸ ë‚´ìš©ì„ ê¸°ì–µí•˜ê¸° ìœ„í•œ ë³€ìˆ˜ ì„ ì–¸
    conversation_history = []

    #########################################################

    # rerank ìˆ˜í–‰ í•¨ìˆ˜
    def rerank(query, documents):
        pairs = [[query, doc] for doc in documents]
        inputs = rerank_tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)
        with torch.no_grad():
            scores = rerank_model(**inputs, return_dict=True).logits.view(-1).float()
        # documentsì™€ scoresë¥¼ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        return sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)

    #########################################################
    
    # ë„¤ì´ë²„ API í˜¸ì¶œ ì—¬ë¶€ í™•ì¸ í•¨ìˆ˜
    def check_if_naver_search_needed(prompt):
        keywords = ['ë§›ì§‘', 'ì¹´í˜', 'ì‹ë‹¹', 'ë ˆìŠ¤í† ë‘', 'ë¸ŒëŸ°ì¹˜ ì¹´í˜', 'í', 'ë°”', 'ìŠ¤ì‹œì§‘', 'ê³ ê¸°ì§‘', 'ë°”ë¹„í', 'í•´ì‚°ë¬¼ ì „ë¬¸ì ', 'í•œì •ì‹', 'ì „í†µ ì¹´í˜']
        modifiers = ['ì‚¬ì§„ ì°ê¸° ì¢‹ì€', 'ì˜¤ì…˜ë·°', 'ì¡°ìš©í•œ', 'ë¶„ìœ„ê¸° ìˆëŠ”', 'ë·°ê°€ ì¢‹ì€', 'í•œì í•œ', 'ì¸ìŠ¤íƒ€ ê°ì„±', 'ê°ì„±ì ì¸', 'ê°€ì¡±ê³¼ í•¨ê»˜', 'ë°ì´íŠ¸í•˜ê¸° ì¢‹ì€', 'ì•„ì´ì™€ í•¨ê»˜', 'ì•¼ê²½ì´ ë©‹ì§„', 'ìì—° ì†', 'ë¡œì»¬ ë§›ì§‘', 'ìˆ¨ì€ ëª…ì†Œ', 'í˜„ì§€ì¸']
    
        # ìˆ˜ì‹ì–´ + í‚¤ì›Œë“œ í˜•íƒœë¡œ ì¡°í•©ì´ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        for keyword in keywords:
            for modifier in modifiers:
                if modifier in prompt and keyword in prompt:
                    print(f"ë„¤ì´ë²„ API í˜¸ì¶œ: ìˆ˜ì‹ì–´ '{modifier}' + í‚¤ì›Œë“œ '{keyword}' ë°œê²¬")
                    return 'ì œì£¼ë„' + ' ' + modifier + ' ' + keyword  # ë‹¨ìˆœí™”ëœ ì¿¼ë¦¬ ë°˜í™˜
        return None
    
    # ë„¤ì´ë²„ API í˜¸ì¶œ í•¨ìˆ˜ 
    def get_naver_results_if_needed(prompt):
        simplified_query = check_if_naver_search_needed(prompt)
        if simplified_query:
            all_results = []
            start = 1
            max_pages = 5  # ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€ ê°€ì ¸ì˜´
            previous_titles = set()  # ì´ì „ì— ê°€ì ¸ì˜¨ ìƒí˜¸ëª…ì„ ì €ì¥í•  ì§‘í•©
            while start <= max_pages * 5:  # í•œ í˜ì´ì§€ë‹¹ 5ê°œì˜ ê²°ê³¼, ì´ 25ê°œì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ì„¤ì •
                encText = urllib.parse.quote(simplified_query)
                print(f"APIì— ì „ë‹¬ëœ ì¿¼ë¦¬: {simplified_query}, í˜ì´ì§€ ì‹œì‘: {start}")
                url = f"https://openapi.naver.com/v1/search/local.json?query={encText}&display=5&start={start}"
                request = urllib.request.Request(url)
                request.add_header("X-Naver-Client-Id", client_id)
                request.add_header("X-Naver-Client-Secret", client_secret)
                response = urllib.request.urlopen(request)
                rescode = response.getcode()
                if rescode == 200:
                    response_body = response.read()
                    result = json.loads(response_body)
                    
                    # ì¤‘ë³µëœ ìƒí˜¸ëª…ì„ í™•ì¸í•˜ê³  í•„í„°ë§
                    new_results = [item for item in result['items'] if item['title'] not in previous_titles]
                    all_results.extend(new_results)
                    
                    # ìƒˆë¡œìš´ ìƒí˜¸ëª…ì„ ì§‘í•©ì— ì¶”ê°€
                    previous_titles.update([item['title'] for item in new_results])
                    
                    print(f"í˜„ì¬ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ ê²°ê³¼ ê°œìˆ˜: {len(new_results)}")
                    if len(new_results) < 5:
                        # ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬
                        break
                    start += 5  # ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°
                else:
                    print(f"Error Code: {rescode}")
                    break
            print(f"ì´ API ê²°ê³¼: {len(all_results)} \n api ê²°ê³¼: {all_results}")
            return all_results
        return None
    
    # ì‹ í•œì¹´ë“œ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ í™•ì¸ í•¨ìˆ˜
    def check_if_shinhancard_related(prompt):
        # ì‹ í•œì¹´ë“œì™€ ê´€ë ¨ëœ í‚¤ì›Œë“œ ëª©ë¡
        shinhancard_keywords = ['ì‹ í•œì¹´ë“œ', 'ì¹´ë“œ í• ì¸', 'ìºì‹œë°±', 'ì œì£¼ ì§ì› ì¶”ì²œ']
        
        # í”„ë¡¬í”„íŠ¸ì— ì‹ í•œì¹´ë“œ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        return any(keyword in prompt for keyword in shinhancard_keywords)
    
    
    #''' ìƒˆë¡œìš´ api ê²°ê³¼ë¥¼ faissì— ì¶”ê°€'''
    def convert_api_data_to_local_format(api_results):
        # API ê²°ê³¼ë¥¼ ë¡œì»¬ ë°ì´í„° í¬ë§·ìœ¼ë¡œ ë³€í™˜
        new_data = []
        for result in api_results:
            new_entry = {
                'ê°€ë§¹ì ëª…': result['title'],
                'ê°€ë§¹ì ì—…ì¢…': result['category'],
                'ê°€ë§¹ì ì£¼ì†Œ': result['address'],
                'text': result['description']
                # í•„ìš” ì‹œ ì¶”ê°€ì ì¸ í•„ë“œë„ ì²˜ë¦¬ ê°€ëŠ¥
            }
            new_data.append(new_entry)
        
        return pd.DataFrame(new_data)
    '''             '''
    
    # ë„¤ì´ë²„ ê²°ê³¼ì™€ ë¡œì»¬ ë°ì´í„° ë¹„êµ (ì •í™•í•œ ì¼ì¹˜ ìš°ì„ , ê·¸ ë‹¤ìŒ í¬í•¨ ì—¬ë¶€)
    def filter_naver_results_with_local_data(naver_results, local_data=df_filtered):
        if naver_results is None:
            return local_data
    
        # ë„¤ì´ë²„ API ê²°ê³¼ì—ì„œ HTML íƒœê·¸ ì œê±°, ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì œê±°, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
        naver_names = [re.sub('<[^<]+?>', '', result['title']).strip().lower() for result in naver_results]
        naver_names = [re.sub(r'[^\w\s]', '', name) for name in naver_names]  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        
        # ë¡œì»¬ ë°ì´í„°ì˜ ê°€ë§¹ì ëª…ë„ ë™ì¼í•˜ê²Œ ì „ì²˜ë¦¬
        local_data['ê°€ë§¹ì ëª…_ì „ì²˜ë¦¬'] = local_data['ê°€ë§¹ì ëª…'].apply(lambda x: re.sub(r'[^\w\s]', '', x).strip().lower())  # íŠ¹ìˆ˜ë¬¸ì ì œê±°, ê³µë°± ì œê±°, ì†Œë¬¸ìë¡œ ë³€í™˜
    
        print(f"ë„¤ì´ë²„ ì „ì²˜ë¦¬ëœ ìƒí˜¸ëª…: {naver_names}")
    
        # 1. ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ë¨¼ì € í•„í„°ë§
        exact_match_df = local_data[local_data['ê°€ë§¹ì ëª…_ì „ì²˜ë¦¬'].isin(naver_names)]
    
        # 2. ì •í™•í•œ ì¼ì¹˜ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í¬í•¨ ì—¬ë¶€ë¡œ í•„í„°ë§
        if exact_match_df.empty:
            filtered_df = local_data[local_data['ê°€ë§¹ì ëª…_ì „ì²˜ë¦¬'].apply(
                lambda local_name: any(local_name in naver_name or naver_name in local_name for naver_name in naver_names)
            )].reset_index(drop=True)
        else:
            filtered_df = exact_match_df.reset_index(drop=True)
    
        print("ë„¤ì´ë²„ APIì™€ ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜ í¬í•¨ë˜ëŠ” ë¡œì»¬ ë°ì´í„°: ", filtered_df)
        return filtered_df
    
    # ì‹ í•œì¹´ë“œ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ í™•ì¸ í•¨ìˆ˜
    def check_if_shinhancard_related(prompt):
        # ì‹ í•œì¹´ë“œì™€ ê´€ë ¨ëœ í‚¤ì›Œë“œ ëª©ë¡
        shinhancard_keywords = ['ì‹ í•œì¹´ë“œ', 'ì¹´ë“œ í• ì¸', 'ìºì‹œë°±', 'ì œì£¼ ì§ì› ì¶”ì²œ']
        
        # í”„ë¡¬í”„íŠ¸ì— ì‹ í•œì¹´ë“œ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        return any(keyword in prompt for keyword in shinhancard_keywords)
    
    
    def extract_location_from_question(question, location_keywords):
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ì§€ëª… í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
        
        Parameters:
        question (str): ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸.
        location_keywords (list): ë°ì´í„°í”„ë ˆì„ì—ì„œ ì¶”ì¶œí•œ ì§€ëª… í‚¤ì›Œë“œ ëª©ë¡ (êµ¬ì—­ ì»¬ëŸ¼ ê°’ë“¤).
        
        Returns:
        str: ì¶”ì¶œëœ ì§€ëª… í‚¤ì›Œë“œ. ì—†ì„ ê²½ìš° None ë°˜í™˜.
        """
        # ì§€ëª… ë’¤ì— ë¶™ì„ ìˆ˜ ìˆëŠ” ì ‘ë¯¸ì‚¬ íŒ¨í„´
        location_suffix_pattern = r"(ì˜|ì—|ì—ì„œ|ì— ìˆëŠ”)?"
    
        question = question.lower().strip()
    
        for location in location_keywords:
            location_pattern = re.escape(location.lower())  # ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ í¬í•¨í•œ ì§€ëª…ì„ escape
            # ì§€ëª… ë’¤ì— ì ‘ë¯¸ì‚¬ê°€ ë¶™ëŠ” ê²½ìš°ì—ë„ ì¼ì¹˜í•˜ë„ë¡ ì •ê·œ í‘œí˜„ì‹ ì‘ì„±
            pattern = rf"\b{location_pattern}{location_suffix_pattern}\b"
            
            if re.search(pattern, question, re.IGNORECASE):
                print(f"ì¶”ì¶œëœ ìœ„ì¹˜ ì •ë³´: {location}")
                return location
        return None
    
    def filter_by_location(df, question, location_column='êµ¬ì—­'):
        """
        ì£¼ì–´ì§„ ì§ˆë¬¸ì— í¬í•¨ëœ ì§€ëª… í‚¤ì›Œë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ì„ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜.
    
        Parameters:
        df (pandas.DataFrame): í•„í„°ë§í•  ë°ì´í„°í”„ë ˆì„
        question (str): ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ í…ìŠ¤íŠ¸
        location_column (str): ì§€ëª…ì´ ì €ì¥ëœ ë°ì´í„°í”„ë ˆì„ì˜ ì—´ ì´ë¦„ (ê¸°ë³¸ê°’: 'êµ¬ì—­')
    
        Returns:
        pandas.DataFrame: í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
        """
        # ë°ì´í„°í”„ë ˆì„ì—ì„œ ì „ì²´ êµ¬ì—­ ì •ë³´ë¥¼ ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        location_keywords = df[location_column].dropna().str.lower().unique()
        
        # # ì „ì²´ ìœ„ì¹˜ ì •ë³´ë¥¼ Streamlit UIì— ì¶œë ¥
        # st.write(f"ì „ì²´ ìœ„ì¹˜ ì •ë³´: {location_keywords}")
        
        # print(f"ì „ì²´ ìœ„ì¹˜ ì •ë³´: {location_keywords}")
    
        # ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì§€ëª… í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
        extracted_location = extract_location_from_question(question, location_keywords)
    
        if extracted_location:
            # ëŒ€ì†Œë¬¸ìì™€ ê³µë°±ì„ ë¬´ì‹œí•˜ê³  í•„í„°ë§
            filtered_df = df[df[location_column].str.contains(extracted_location, case=False, na=False)].reset_index(drop=True)
            return filtered_df  # ì¼ì¹˜í•˜ëŠ” ì§€ëª…ì´ ìˆìœ¼ë©´ í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
    
        return df
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥ ë° ëŒ€í™” í„´ ê´€ë¦¬ í•¨ìˆ˜
    def add_to_conversation_history(question, response_text=None):
        if "conversation_history" not in st.session_state:
            st.session_state.conversation_history = []

        # ì§ˆë¬¸ ì¶”ê°€
        st.session_state.conversation_history.append(f"ì§ˆë¬¸: {question}")
        
        # ì‘ë‹µì´ ìˆë‹¤ë©´ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        if response_text:
            st.session_state.conversation_history.append(f"ì‘ë‹µ: {response_text}")
    
        # í˜ë¥´ì†Œë‚˜ë³„ë¡œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
    def generate_personalized_prompt(persona, question, reference_info, recommendation_message):
        # ëŒ€í™” ê¸°ë¡ì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ë¶ˆëŸ¬ì˜¤ê¸°
        conversation_history = "\n".join(st.session_state.conversation_history) if "conversation_history" in st.session_state else ""
    

        # ëŒ€í™” ê¸°ë¡ê³¼ í˜ë¥´ì†Œë‚˜ë¥¼ ë°˜ì˜í•œ ë‹µë³€ ìƒì„±
        if persona == "í•´ë…€":
            return (
                f"ë„ˆëŠ” ì§€ê¸ˆë¶€í„° í•´ë…€ì´ê³  ë‚˜í•œí…Œ ì œì£¼ë„ ë°©ì–¸ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ë§ë¡œ ë§›ì§‘ ì¶”ì²œì„ í•´ì¤„ê±°ì•¼.\n"
                f"ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤:\n{conversation_history}\n,"
                f"ìƒˆë¡œìš´ ì§ˆë¬¸: {question}\n ì¶”ì²œí•  ë§›ì§‘ì˜ ì •ë³´: \n{reference_info}\n,"
                f"ëŒ€í™” ê¸°ë¡ì€ ì·¨í–¥ë§Œ ê³ ë ¤í•˜ê³ , ìƒˆë¡œìš´ ì§ˆë¬¸ê³¼ ì¶”ì²œí•  ë§›ì§‘ì˜ ì •ë³´ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ì œì£¼ í•´ë…€ê°€ ì œì£¼ë„ ë°©ì–¸ì„ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼í•˜ê²Œ 'ë°˜ë§'ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤: {recommendation_message}"
            )
        elif persona == "í•˜ë¥´ë°©":
            return (
                f"ë„ˆëŠ” ì§€ê¸ˆë¶€í„° í•˜ë¥´ë°©ì´ê³  ë‚˜í•œí…Œ ì œì£¼ë„ ë°©ì–¸ì„ ì‚¬ìš©í•˜ì—¬ ì¡´ëŒ“ë§ë¡œ ë§›ì§‘ ì¶”ì²œì„ í•´ì¤„ê±°ì•¼.\n"
                f"ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤:\n{conversation_history}\n"
                f"ìƒˆë¡œìš´ ì§ˆë¬¸: {question}\n ì¶”ì²œí•  ë§›ì§‘ì˜ ì •ë³´: \n{reference_info}\n"
                f"ëŒ€í™” ê¸°ë¡ì€ ì·¨í–¥ë§Œ ê³ ë ¤í•˜ê³ , ìƒˆë¡œìš´ ì§ˆë¬¸ê³¼ ì¶”ì²œí•  ë§›ì§‘ì˜ ì •ë³´ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ì œì£¼ í•˜ë¥´ë°©ì´ ì œì£¼ë„ ë°©ì–¸ì„ ì‚¬ìš©í•˜ì—¬ ì ì–ê²Œ ì¡´ëŒ“ë§ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤: {recommendation_message}"
            )
        elif persona == "ì œì£¼ì†Œë…€":
            return (
                f"ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ì œì£¼ì†Œë…€ì´ê³  ë‚˜í•œí…Œ í‘œì¤€ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡´ëŒ“ë§ë¡œ ë§›ì§‘ ì¶”ì²œì„ í•´ì¤„ê±°ì•¼.\n"
                f"ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤:\n{conversation_history}\n"
                f"ìƒˆë¡œìš´ ì§ˆë¬¸: {question}\n ì¶”ì²œí•  ë§›ì§‘ì˜ ì •ë³´: \n{reference_info}\n"
                f"ëŒ€í™” ê¸°ë¡ì€ ì·¨í–¥ë§Œ ê³ ë ¤í•˜ê³ , ìƒˆë¡œìš´ ì§ˆë¬¸ê³¼ ì¶”ì²œí•  ë§›ì§‘ì˜ ì •ë³´ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ì œì£¼ ì†Œë…€ê°€ í‘œì¤€ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡´ëŒ“ë§ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤: {recommendation_message}"
            )
        elif persona == "ì œì£¼ì†Œë…„":
            return (
                f"ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ì œì£¼ì†Œë…„ì´ê³  ë‚˜í•œí…Œ í‘œì¤€ì–´ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ë§ë¡œ ë§›ì§‘ ì¶”ì²œì„ í•´ì¤„ê±°ì•¼.\n"
                f"ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤:\n{conversation_history}\n"
                f"ìƒˆë¡œìš´ ì§ˆë¬¸: {question}\n ì¶”ì²œí•  ë§›ì§‘ì˜ ì •ë³´: \n{reference_info}\n"
                f"ëŒ€í™” ê¸°ë¡ì€ ì·¨í–¥ë§Œ ê³ ë ¤í•˜ê³ , ìƒˆë¡œìš´ ì§ˆë¬¸ê³¼ ì¶”ì²œí•  ë§›ì§‘ì˜ ì •ë³´ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ì œì£¼ ì†Œë…„ì´ í‘œì¤€ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹œêµ¬ì²˜ëŸ¼ ì¶”ì²œí•©ë‹ˆë‹¤: {recommendation_message}"
            )
    # generate_response_with_faiss_and_naver í•¨ìˆ˜ ìˆ˜ì •
    def generate_response_with_faiss_and_naver(question, df_filtered, embeddings, model, embed_text, franchise, filtered_faiss_index, k=3, print_prompt=True):
        # ë„¤ì´ë²„ API í˜¸ì¶œì´ í•„ìš”í•œì§€ í™•ì¸
        api_results = get_naver_results_if_needed(question)

        # ì‹ í•œì¹´ë“œ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        is_shinhancard_related = check_if_shinhancard_related(question)

        # FAISS ì¸ë±ìŠ¤ë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
        query_embedding = embed_text(question).reshape(1, -1)
        distances, indices = filtered_faiss_index.search(query_embedding, k * 5)  # ìœ ì‚¬ë„ ê¸°ì¤€ ìƒìœ„ 5ë°°ìˆ˜ ê²€ìƒ‰
        print(f"FAISS ê²€ìƒ‰ ê²°ê³¼ ì¸ë±ìŠ¤: {indices}")
        print(f"ë°ì´í„°í”„ë ˆì„ í–‰ ê°œìˆ˜: {len(df)}")
        faiss_search_results = df_filtered.iloc[indices[0, :]].reset_index(drop=True)

        # í”„ëœì°¨ì´ì¦ˆ í¬í•¨ ì˜µì…˜ í•„í„°ë§
        if franchise == 'í”„ëœì°¨ì´ì¦ˆ ë¯¸í¬í•¨':
            faiss_search_results = faiss_search_results[faiss_search_results['í”„ëœì°¨ì´ì¦ˆìœ ë¬´'] == 0].reset_index(drop=True)
        elif franchise == 'í”„ëœì°¨ì´ì¦ˆ í¬í•¨':
            faiss_search_results = faiss_search_results[faiss_search_results['í”„ëœì°¨ì´ì¦ˆìœ ë¬´'] == 1].reset_index(drop=True)

        # ì§€ëª… í•„í„°ë§ ì ìš©
        df_filtered_by_location = filter_by_location(df_filtered, question)

        # ì‹ í•œì¹´ë“œ ì¶”ì²œ ì—¬ë¶€ì— ë”°ë¥¸ í•„í„°ë§
        if is_shinhancard_related:
            shinhancard_recommended_df = df_filtered_by_location[df_filtered_by_location['ì‹ í•œì¹´ë“œì¶”ì²œ'] == 1].reset_index(drop=True)
            if not shinhancard_recommended_df.empty:
                filtered_df = shinhancard_recommended_df
            else:
                filtered_df = df_filtered_by_location
        else:
            if api_results:
                filtered_api_data = filter_naver_results_with_local_data(api_results, df_filtered_by_location)
                if not filtered_api_data.empty:
                    new_data_df = add_new_api_results_to_faiss(api_results, filtered_faiss_index, df_filtered_by_location)
                    if not new_data_df.empty:
                        df_filtered_by_location = pd.concat([df_filtered_by_location, new_data_df]).reset_index(drop=True)
                    filtered_df = filtered_api_data
                else:
                    return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."
            else:
                filtered_df = df_filtered_by_location.head(k)

        # ê²°ê³¼ê°€ ì—†ì„ ë•Œ ì²˜ë¦¬
        if filtered_df.empty:
            return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

        # ì‹ í•œì¹´ë“œ ì¶”ì²œ ë©˜íŠ¸ ì¶”ê°€
        if is_shinhancard_related and filtered_df['ì‹ í•œì¹´ë“œì¶”ì²œ'].any():
            recommendation_message = "ì œì£¼ ì—¬í–‰ì€ ì‹ í•œì¹´ë“œë¡œ~ ì œì£¼ ì§ì›ì´ ì¶”ì²œí•œ ë§›ì§‘ë§Œ ê°€ë„ 20% ìºì‹œë°±!"
        else:
            recommendation_message = ""

        # ê¸°ì¡´ ì½”ë“œ
        # ì°¸ê³ í•  ì •ë³´ ìƒì„±
        # reference_info = "\n".join([f"{row['text']}" for idx, row in filtered_df.iterrows()])

        #############################################

        # FAISS ê²°ê³¼ì™€ í•„í„°ë§ëœ ë°ì´í„° ê²°í•©
        combined_results = pd.concat([faiss_search_results, filtered_df]).drop_duplicates().reset_index(drop=True)

        # combined_resultsì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ rerank ì ìš©
        search_texts = [row['text'] for idx, row in combined_results.iterrows()]
        reranked_results = rerank(question, search_texts)
        reranked_texts = [doc[0] for doc in reranked_results[:k]]  # ìƒìœ„ kê°œì˜ rerank ê²°ê³¼ë§Œ ì‚¬ìš©

        # ì°¸ê³ í•  ì •ë³´ ìƒì„±
        reference_info = "\n".join(reranked_texts)

        #############################################
        
        # í˜ë¥´ì†Œë‚˜ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
        persona = st.session_state.local_choice
        prompt = generate_personalized_prompt(persona, question, reference_info, recommendation_message)

        if print_prompt:
            print('-----------------------------' * 3)
            print(prompt)
            print('-----------------------------' * 3)

        # LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response = model.generate_content(prompt)

        return response

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
    
    # ìƒˆë¡œìš´ ì‘ë‹µ ìƒì„±
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = generate_response_with_faiss_and_naver(prompt, df_filtered, embeddings, model, embed_text, franchise, filtered_faiss_index)
                placeholder = st.empty()
                
                try:
                    # ì‘ë‹µì´ ë¬¸ìì—´ì¼ ë•Œì™€ ì•„ë‹ ë•Œë¥¼ ë¶„ê¸° ì²˜ë¦¬
                    if isinstance(response, str):
                        full_response = response
                    else:
                        # response.text ì†ì„±ì— ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                        full_response = getattr(response, 'text', 'Error: Response does not have text attribute')
                    
                    # ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                    add_to_conversation_history(prompt, full_response)
                    placeholder.markdown(full_response)
                except AttributeError as e:
                    st.error(f"Error occurred: {e}")
                
        st.session_state.messages.append({"role": "assistant", "content": full_response})

        
# CSS ìŠ¤íƒ€ì¼ ì¶”ê°€
st.markdown("""
<style>
    div.stButton > button {
        display: block;
        margin: 0 auto;
    }
    h1 {
        font-size: 32px;  /* ì›í•˜ëŠ” ê¸€ì í¬ê¸°ë¡œ ë³€ê²½ */
        color: #000000;  /* ê¸€ì ìƒ‰ìƒ ë³€ê²½ (ì›í•˜ëŠ” ìƒ‰ìƒìœ¼ë¡œ) */
        margin: 0;  /* ê¸°ë³¸ ë§ˆì§„ ì œê±° */
    }
    .header {
        background-color: #E0ECF8;  /* í—¤ë” ë°°ê²½ìƒ‰ ì§€ì • */
        padding: 20px;  /* í—¤ë” íŒ¨ë”© ì¡°ì • */
        margin: 20px;  /* í—¤ë” ë°”ê¹¥ìª½ ì—¬ë°± ì¡°ì • (ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ë³€ê²½) */
        border-radius: 8px;  /* í—¤ë” ëª¨ì„œë¦¬ ë‘¥ê¸€ê²Œ ì„¤ì • */
        text-align: center;  /* í—¤ë” ë‚´ìš© ì¤‘ì•™ ì •ë ¬ */
    }
    p {
        font-size: 18px;  /* ì›í•˜ëŠ” ê¸€ì í¬ê¸°ë¡œ ë³€ê²½ */
        color: #000000;  /* ê¸€ì ìƒ‰ìƒ ë³€ê²½ (ì›í•˜ëŠ” ìƒ‰ìƒìœ¼ë¡œ) */
    }
</style>
""", unsafe_allow_html=True)
